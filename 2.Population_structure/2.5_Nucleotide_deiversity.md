
## Step 1: Run bash script to estimate nucleotide diversity per population and morphs
```{r}
#!/bin/bash -l

#SBATCH -A uppmax2025-2-114
#SBATCH -p node
#SBATCH -n 1
#SBATCH -C mem256GB
#SBATCH -t 05-00:00:00
#SBATCH --array=1-40
#SBATCH -J ND
#SBATCH -e ND_%A_%a.err
#SBATCH -o ND_%A_%a.out
#SBATCH --mail-type=all
#SBATCH --mail-user=khrystyna.kurta@slu.se

#Load modules
module load bioinfo-tools
module load ANGSD/0.933
module load R_packages/4.3.1


#Set minimum depth
MIN_DEPTH=1

# Set it at 2-4 times the expected coverage to remove repeated regions
MAX_DEPTH_FACTOR=3

#Set the CPU as in the header to slurm
NB_CPU=8

#Rscript
rscript=/proj/snic2020-2-19/private/herring/users/khrystyna/Arctic_charr_ref_fSalAlp1/Nucleotide_diversity_NSITES/Rscripts/sum_sites_sfs.r

#Path to the directory where you have the bam-files
BASEDIR=/proj/snic2020-2-19/private/herring/users/khrystyna/Arctic_charr_ref_fSalAlp1/Run_1_2

#Text file containing sample bam paths
BAM_LIST=/proj/snic2020-2-19/private/herring/users/khrystyna/Arctic_charr_ref_fSalAlp1/BAM_LISTS/FST_list

#Reference genome
REFGENOME=/proj/snic2020-2-19/private/arctic_charr/assemblies/fSalAlp1.1/Data_Package_fSalAlp1_assembly_20231024/fSalAlp1.1.hap1.cur.20231016.fasta
REF_INDEXED=/proj/snic2020-2-19/private/arctic_charr/assemblies/fSalAlp1.1/Data_Package_fSalAlp1_assembly_20231024/fSalAlp1.1.hap1.cur.20231016.fasta.fai

##STEP 2: Determine chromosome/ or Get all the contig (or scaffold) names from the reference genome fasta file

CHUNK_DIR=/proj/snic2020-2-19/private/herring/users/khrystyna/Arctic_charr_ref_fSalAlp1/CHUNK_LIST
CHUNK_NAMES=/proj/snic2020-2-19/private/herring/users/khrystyna/Arctic_charr_ref_fSalAlp1/CHUNK_LIST/chr_ordered_txt.list
CHUNK_NAMES_target=$(cat $CHUNK_NAMES | sed -n ${SLURM_ARRAY_TASK_ID}p)
CHUNK_NAMES_target_name=${CHUNK_NAMES_target/.txt/}

# Number of sites to use in realSFS (for faster computation)
NSITES=500000

# Output directory
OUT_DIR=/proj/snic2020-2-19/private/herring/users/khrystyna/Arctic_charr_ref_fSalAlp1/Nucleotide_diversity_NSITES

# Set minimum read depth per individual
MIN_DEPTH=1

# Depth threshold factor for max depth filtering (e.g., 3x expected coverage)
MAX_DEPTH_FACTOR=3

# Number of CPUs to use
NB_CPU=20

# Path to R script that summarizes 2D SFS
rscript=/proj/snic2020-2-19/private/herring/users/khrystyna/Arctic_charr_ref_fSalAlp1/FST_MAF0.05_FOLDED/Rscripts/01_sum_sites_2dsfs.r


#      INPUT FILE PATHS      #


# Directory with scaffold/contig chunk lists
CHUNK_DIR=/proj/snic2020-2-19/private/herring/users/khrystyna/Arctic_charr_ref_fSalAlp1/CHUNK_LIST

# Target scaffold/chunk from SLURM array
CHUNK_NAMES=/proj/snic2020-2-19/private/herring/users/khrystyna/Arctic_charr_ref_fSalAlp1/CHUNK_LIST/chr_ordered_txt.list
CHUNK_NAMES_target=$(sed -n ${SLURM_ARRAY_TASK_ID}p $CHUNK_NAMES)
CHUNK_NAMES_target_name=${CHUNK_NAMES_target/.txt/}

# BAM list directory
BAM_LIST=/proj/snic2020-2-19/private/herring/users/khrystyna/Arctic_charr_ref_fSalAlp1/BAM_LISTS/FST_list

# Reference genome and index
REFGENOME=/proj/snic2020-2-19/private/arctic_charr/assemblies/fSalAlp1.1/Data_Package_fSalAlp1_assembly_20231024/fSalAlp1.1.hap1.cur.20231016.fasta
REF_INDEXED=${REFGENOME}.fai

# Go to working directory
cd $BASEDIR


#     RUN PER POPULATION     #


for POP in $BAM_LIST/pop*list; do
  OUTPUT=$(basename $POP)
  N_IND=$(wc -l < $POP)

  # Determine percentage threshold based on sample size
  if [ "$N_IND" -lt 12 ]; then
    PERCENT_IND=0.9
  else
    PERCENT_IND=0.5
  fi

  # Compute min number of individuals and max depth
  MIN_IND=$(printf "%.0f" $(echo "$N_IND * $PERCENT_IND" | bc))
  MAX_DEPTH=$(echo "$N_IND * $MAX_DEPTH_FACTOR" | bc)

  echo "Processing $OUTPUT ($N_IND individuals)"
  echo " - PERCENT_IND=$PERCENT_IND → MIN_IND=$MIN_IND"

  # ------------------------------
  # STEP 1: Estimate SAF file
  # ------------------------------
  angsd -P $NB_CPU -underFlowProtect 1 \
    -dosaf 1 -GL 2 -doMajorMinor 1 -doCounts 1 \
    -anc $REFGENOME -fai $REF_INDEXED \
    -rf $CHUNK_DIR/$CHUNK_NAMES_target \
    -remove_bads 1 -minMapQ 30 -minQ 20 -uniqueOnly 1 \
    -minInd $MIN_IND -setMaxDepth $MAX_DEPTH -setMinDepthInd $MIN_DEPTH \
    -bam $POP \
    -out $OUT_DIR/${OUTPUT}_${CHUNK_NAMES_target_name}_pctind${PERCENT_IND}_maxdepth${MAX_DEPTH}

  # ------------------------------
  # STEP 2: Estimate SFS
  # ------------------------------
  echo "Estimating real SFS (max $NSITES sites)"
  realSFS $OUT_DIR/${OUTPUT}_${CHUNK_NAMES_target_name}_pctind${PERCENT_IND}_maxdepth${MAX_DEPTH}.saf.idx \
    -P $NB_CPU -nSites $NSITES -maxIter 50 \
    > $OUT_DIR/${OUTPUT}_${CHUNK_NAMES_target_name}_pctind${PERCENT_IND}_maxdepth${MAX_DEPTH}.${NSITES}.sfs

  # ------------------------------
  # STEP 3: Summarize 2D SFS using R
  # ------------------------------
  file=$OUT_DIR/${OUTPUT}_${CHUNK_NAMES_target_name}_pctind${PERCENT_IND}_maxdepth${MAX_DEPTH}.${NSITES}
  Rscript $rscript "$file"

  # ------------------------------
  # STEP 4: Estimate Thetas
  # ------------------------------
  echo "Estimating Theta values for $POP"
  angsd -P $NB_CPU -dosaf 1 -doThetas 1 -GL 2 -doMajorMinor 1 -underFlowProtect 1 \
    -anc $REFGENOME -fai $REF_INDEXED -remove_bads 1 -minMapQ 30 -minQ 20 \
    -minInd $MIN_IND -setMinDepthInd $MIN_DEPTH \
    -rf $CHUNK_DIR/$CHUNK_NAMES_target \
    -bam $POP \
    -pest $file.dsfs \
    -out $file

  # ------------------------------
  # STEP 5: Theta Summary & Sliding Window
  # ------------------------------

  # Genome-wide theta summary
  thetaStat do_stat $file.thetas.idx

  # Sliding window estimate
  thetaStat do_stat $file.thetas.idx -win 20000 -step 10000 \
    -outnames $file.thetaswindow

done

```
### R script 01_sum_sites_2dsfs.r
```{r}
# code R to take the sfs made on a subsample of sites in a usable format for subsequent analyses
argv <- commandArgs(T)
file<-argv[1]


sfs<-read.table (paste0(file))
sfs.sum<-colSums(sfs)
write.table(rbind(sfs.sum),  quote=F, col.names=F, row.names=F,paste0(file, ".dsfs"))
```

## Step 2: Compute summary statistics for nucleotide diversity per every lake and morphs within lakes in R
```{r}

#Set up

library(tidyverse)
library(patchwork)
library(zoo)
library(data.table)


#Directory
dir <- '~/Desktop/Comp_UU/REF_SalAlp_UK/Nucleotide_diversity/MORPH_PG'

#Load data 
files <- list.files(dir)

# Filter files that start with 5kb
desired_files <- grep("pestPG$", files, value = TRUE)
col_names <- c('Chr',	'WinCenter',	'tW',	'tP',	'tF',	'tH',	'tL',	'Tajima', 'fuf',
               'fud',	'fayh',	'zeng',	'nSites')

# Read the desired files
thetas_list <- list()

for (file in desired_files) {
  file_path <- file.path(dir, file)
  
  # Check with readLines — skip if no lines or only header
  lines <- readLines(file_path, n = 2, warn = FALSE)
  if (length(lines) < 2) {
    message(sprintf("Skipping empty or header-only file: %s", file))
    next
  }
  
  # Safe read
  file_content <- read.table(file_path, header = FALSE)[, -1]
  colnames(file_content) <- col_names
  
  # Store in list
  thetas_list[[file]] <- file_content
}

#Modify names of the data list 
names(thetas_list) <- gsub("_pctind0.5_maxdepth.*\\.pestPG$", '', names(thetas_list) )

# List of prefixes to search for
prefixes <- c("maf_Myv_pca_DB", 'maf_Myv_pca_Pi',
              "maf_SirDB", "maf_SirPL", 
              "maf_Van_DB","maf_Van_PL", 
              "maf_ThiPi","maf_ThiPL",  
              "maf_ThiLB","maf_ThiSB",
              "maf_Thi_LB_SB", "maf_Thi_Pi_PL",
              'pop_Myv', 'pop_Sir', 
              'pop_Van', 'pop_Thin'
              )  # Add more prefixes as needed

# Initialize a new list to store combined data frames
new_data_list <- list()

# Define a function to bind rows of data frames starting with a similar name
for(prefix in prefixes) {
  # Filter data frames starting with the specified prefix
  matching_data_frames <- grep(paste0("^", prefix), names(thetas_list), value = TRUE)
  
  # If there are matching data frames, bind them row-wise
  if (length(matching_data_frames) > 0) {
    combined_data <- do.call(rbind, thetas_list[matching_data_frames])
    new_data_list[[prefix]] <- combined_data
  }
}


# Estimate Pi and Watterson’s theta  -------------------------------------------
l.theta <- list()
l.watt <- list()
l.sum <- list()
l.dfs <- list()

for (pop in seq_along(new_data_list)){
  df.theta <- new_data_list[[pop]]
  pop.name <- names(new_data_list[pop])
  l.theta[[pop.name]] <- mean((as.numeric(df.theta$tP) / as.numeric(df.theta$nSites)), na.rm = T)
  l.watt[[pop.name]] <- mean((as.numeric(df.theta$tW) /as.numeric(df.theta$nSites)), na.rm = T)
  l.sum[[pop.name]] <- sum(as.numeric(df.theta$nSites), na.rm = T)
  df.theta$pop <- pop.name
  l.dfs[[pop.name]] <- df.theta
}

length(l.theta) #as the number of populations
length(l.watt)  #as the number of populations

df.x2 <- data.frame(pop = names(l.theta),
                    pairwise.nuc = round(unlist(l.theta), 4),  
                    wattersons.theta = round(unlist(l.watt), 4),
                    number.sites = round(unlist(l.sum), 4))

mean.theta.nsites <- df.x2
df.theta.nsites <- bind_rows(l.dfs)
names(df.theta.nsites) <- c("Chr", "WinCenter","tW", "tP","tF","tH","tL","Tajima","fuf","fud","fayh","zeng","nSites", "pop")


#Cleaner summary per lake and per morph
#Filter windows > 10 sites for higher accuracy
df.theta.nsites$nSites <- as.numeric(df.theta.nsites$nSites)
df.theta.nsites$tP <- as.numeric(df.theta.nsites$tP)
df.theta.nsites$tW <- as.numeric(df.theta.nsites$tW)

clean.summary.thetas <- 
  df.theta.nsites %>% 
  filter(nSites > 10) %>%
  group_by(pop) %>%
  summarise(mean.pi = round(mean(tP / nSites),4),
            sd.pi = round(sd(tP / nSites),4),
            mean.wat = round(mean(tW / nSites),4),
            sd.wat = round(sd(tW / nSites),4)) %>%
  mutate(out.pi = paste0(mean.pi,"±",sd.pi),
         out.wat = paste0(mean.wat,"±",sd.wat),)

print(clean.summary.thetas)
```

