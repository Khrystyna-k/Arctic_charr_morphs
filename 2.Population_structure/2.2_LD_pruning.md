
## Step 1: LD Pruning and Genotype Likelihood Estimation with ANGSD and PLINK
```{r}
#!/bin/bash -l
#SBATCH -A uppmax2025-2-114
#SBATCH -p core
#SBATCH -n 20
#SBATCH --array=1-40
#SBATCH -t 08-00:00:00
#SBATCH -J LD_prunning
#SBATCH -e LD_prunning_%A_%a.err
#SBATCH -o LD_prunning_%A_%a.out
#SBATCH --mail-type=all
#SBATCH --mail-user=khrystyna.kurta@slu.se

## Load required modules
module load bioinfo-tools
module load ANGSD/0.933
module load plink/1.90b4.9

## Set base directory
BASEDIR=/.../herring/users/khrystyna/Arctic_charr_ref_fSalAlp1/Run_1_2
cd $BASEDIR

## Reference genome and index
REFGENOME=/.../arctic_charr/assemblies/fSalAlp1.1/...fasta
REF_INDEXED=${REFGENOME}.fai

## BAM list
BAMLIST=all_bam.list

## Output file base name
OUT_NAME=All

## Sites file (pre-filtered SNPs)
SITES=/.../SITES/sites_sorted.txt

## Output directory
OUTDIR=/.../LD_PRUNNED

## Analysis settings
NB_CPU=8
MIN_DEPTH=1
MAX_DEPTH_FACTOR=3
MIN_MAF=0.05
PERCENT_IND=0.5
N_IND=$(cat $BAMLIST | wc -l)
MIN_IND_FLOAT=$(echo "$N_IND * $PERCENT_IND" | bc -l)
MIN_IND=${MIN_IND_FLOAT%.*}
MAX_DEPTH=$(echo "$N_IND * $MAX_DEPTH_FACTOR" | bc -l)

## Chromosome index (per SLURM task)
CHR=${SLURM_ARRAY_TASK_ID}

## Run ANGSD to generate PLINK files for each genomic region
angsd -P $NB_CPU -doPlink 2 -doMaf 1 -doCounts 1 \
  -GL 2 -doGeno -2 -doMajorMinor 4 -doPost 1 -postCutoff 0.8 \
  -ref $REFGENOME -fai $REF_INDEXED \
  -r $CHR \
  -minInd $MIN_IND -setMaxDepth $MAX_DEPTH -setMinDepthInd $MIN_DEPTH \
  -uniqueOnly 1 -remove_bads 1 -only_proper_pairs 1 -minMapQ 30 -minQ 20 \
  -sites $SITES \
  -bam $BAMLIST \
  -out $OUTDIR/${OUT_NAME}_maf${MIN_MAF}_pctind${N_IND}_maxdepth${MAX_DEPTH}_region${CHR}

## Combine TPED files from all regions
ls $OUTDIR/*region*.tped | sort -V > $OUTDIR/tped_list.txt
cat $(cat $OUTDIR/tped_list.txt) > $OUTDIR/all_regions.tped
cp $OUTDIR/*region*.tfam $OUTDIR/all_regions.tfam  ## Use any .tfam file

## Perform LD pruning with PLINK
WINDOW=100
SNP=5
VIF=2
plink --tped $OUTDIR/all_regions.tped \
  --tfam $OUTDIR/all_regions.tfam \
  --indep $WINDOW $SNP $VIF --allow-extra-chr --chr-set 40 \
  --out $OUTDIR/all_regions.pruned --threads $NB_CPU

## Convert pruned SNP list into ANGSD-compatible sites file
ANGSD_OUT=/.../SITES/angsd_sites_sorted_prunned.txt
awk -F"_" '{print $1, $2}' all_regions.pruned.prune.in > $ANGSD_OUT
angsd sites index $ANGSD_OUT
echo "N: $(cat $ANGSD_OUT | wc -l) pruned sites are saved"
#STEP 3: Run angsd again, with LD pruned sites
```

## Step 2: Run ANGSD on LD-pruned sites to generate genotype likelihoods
```{r}
#!/bin/bash -l
#SBATCH -A uppmax2025-2-114
#SBATCH -p node
#SBATCH -n 1
#SBATCH -C mem256GB
#SBATCH --array=1-40
#SBATCH -t 04-00:00:0
#SBATCH -J beagle_prune
#SBATCH -e beagle_prune_%A_%a.err
#SBATCH -o begle_prune_%A_%a.out
#SBATCH --mail-type=all
#SBATCH --mail-user=khrystyna.kurta@slu.se

#Load modules
module load bioinfo-tools
module load ANGSD/0.933


## CONDUCT GENOTYPE LIKELIHOOD ESTIMATION USING ANGSD

#STEP 1: Specify out directory
BASEDIR=/.../herring/users/khrystyna/Arctic_charr_ref_fSalAlp1/Run_1_2
OUTDIR=/.../herring/users/khrystyna/Arctic_charr_ref_fSalAlp1/GL_MAF_GENO_PRUNNED


#STEP 2: Define paths to Reference genome
REFGENOME=/.../arctic_charr/assemblies/fSalAlp1.1/Data_Package_fSalAlp1_assembly_20231024/fSalAlp1.1.hap1.cur.20231016.fasta
REF_INDEXED=/.../arctic_charr/assemblies/fSalAlp1.1/Data_Package_fSalAlp1_assembly_20231024/fSalAlp1.1.hap1.cur.20231016.fasta.fai


CHUNK_DIR=/.../herring/users/khrystyna/Arctic_charr_ref_fSalAlp1/CHUNK_LIST
CHUNK_NAMES=/.../herring/users/khrystyna/Arctic_charr_ref_fSalAlp1/CHUNK_LIST/only_chr.list
CHUNK_NAMES_target=$(cat $CHUNK_NAMES | sed -n ${SLURM_ARRAY_TASK_ID}p)
CHUNK_NAMES_target_name=${CHUNK_NAMES_target/.txt/}


#STEP 3: Specify PATH to prunned sites 
SITES=/.../herring/users/khrystyna/Arctic_charr_ref_fSalAlp1/SITES/angsd_sites_sorted_prunned.txt

#STEP 4: Specify which bam list to use
BAM_LIST_PATH=/.../herring/users/khrystyna/Arctic_charr_ref_fSalAlp1/BAM_LISTS/BEAGLE_PRUNNED 

BAM_LIST=$BAM_LIST_PATH/pop_All.list
BAM_TARGET=${BAM_LIST/.list/}
OUTPUT=$(basename $BAM_TARGET)


#Go to dir
cd $BASEDIR

#STEP 4: Run ANGSD
angsd -bam "$BAM_LIST" \
-ref "$REFGENOME" -fai "$REF_INDEXED" \
-rf $CHUNK_DIR/${CHUNK_NAMES_target}.txt \
-remove_bads 1 -only_proper_pairs 1 -minMapQ 30 -minQ 20 -uniqueOnly 1 \
-GL 2 -doMajorMinor 4 -doMaf 1 -doPost 2 -doGlf 2 \
-sites "$SITES" \
-out $OUTDIR/${OUTPUT}_${CHUNK_NAMES_target_name}_DEPTH_MAF0.05_MapQ30_prunned \
-nThreads 10


```

# Step 3: Merge all chromosomes
```{r}
#!/bin/bash -l
#SBATCH -A uppmax2025-2-114
#SBATCH -p core
#SBATCH -n 4
#SBATCH -t 04-00:00:0
#SBATCH -J beagle_prune
#SBATCH -e beagle_prune_%A_%a.err
#SBATCH -o begle_prune_%A_%a.out
#SBATCH --mail-type=all
#SBATCH --mail-user=khrystyna.kurta@slu.se


cd /proj/snic2020-2-19/private/herring/users/khrystyna/Arctic_charr_ref_fSalAlp1/GL_MAF_GENO_PRUNNED

## Step 1: Extract header from first file
zcat pop_All_*_DEPTH_MAF0.05_MapQ30_prunned.beagle.gz | head -n 1 > header.txt

## Step 2: Decompress all files, remove headers, and merge body into one temp file
> body.txt  ## initialize/empty the file

for f in pop_All_*_DEPTH_MAF0.05_MapQ30_prunned.beagle.gz; do
    echo "Processing $f"
    zcat "$f" | tail -n +2 >> body.txt
done

## Step 3: Combine header and body, then compress final beagle file
cat header.txt body.txt | gzip > All_allCHR_DEPTH_MAF0.05_MapQ30_prunned.beagle.gz

## Step 4: Clean up
rm header.txt body.txt

```
