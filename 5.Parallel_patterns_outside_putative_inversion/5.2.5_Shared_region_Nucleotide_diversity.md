
# Step 1: Run bash script to estimate nucleotide diversity 
```{r}
#!/bin/bash -l

#SBATCH -A uppmax2025-2-114
#SBATCH -p core -n 6
#SBATCH -t 08-00:00:00
#SBATCH --array=1-4
#SBATCH -J ND_shared
#SBATCH -e ND_shared_%A_%a.err
#SBATCH -o ND_shared_%A_%a.out
#SBATCH --mail-type=all
#SBATCH --mail-user=khrystyna.kurta@slu.se

#Load modules
module load bioinfo-tools
module load ANGSD/0.933
module load R_packages/4.0.4

#SET UP CHUNK list
#Specify which bam to use
CHUNK_DIR=/proj/snic2020-2-19/private/herring/users/khrystyna/Arctic_charr_ref_fSalAlp1/CHUNK_LIST


##STEP 2: Determine chromosome/ or Get all the contig (or scaffold) names from the reference genome fasta file
#SET UP CHUNK list
CHUNK_DIR=/proj/snic2020-2-19/private/herring/users/khrystyna/Arctic_charr_ref_fSalAlp1/CHUNK_LIST

CHUNK_NAMES=$CHUNK_DIR/34.txt
CHUNK_NAMES_target=$(basename $CHUNK_NAMES)
CHUNK_NAMES_target_name=${CHUNK_NAMES_target/.txt/}


#Path to the directory where you have the bam-files
BASEDIR=/proj/snic2020-2-19/private/herring/users/khrystyna/Arctic_charr_ref_fSalAlp1/Run_1_2
cd $BASEDIR

#Text file containing sample bam paths

BAM_LIST=/proj/snic2020-2-19/private/herring/users/khrystyna/Arctic_charr_ref_fSalAlp1/BAM_LISTS/Nucleotide_diversity_list/

#Reference genome
REFGENOME=/proj/snic2020-2-19/private/arctic_charr/assemblies/fSalAlp1.1/Data_Package_fSalAlp1_assembly_20231024/fSalAlp1.1.hap1.cur.20231016.fasta
REF_INDEXED=/proj/snic2020-2-19/private/arctic_charr/assemblies/fSalAlp1.1/Data_Package_fSalAlp1_assembly_20231024/fSalAlp1.1.hap1.cur.20231016.fasta.fai

#Create a directory
OUT_DIR=/proj/snic2020-2-19/private/herring/users/khrystyna/Arctic_charr_ref_fSalAlp1/Nucleotide_diversity

#Step 1: 
MIN_DEPTH=1
MAX_DEPTH_FACTOR=3

#To make the realsSFS go faster -reduce the number of sites
NSITES=500000

#Set the CPU as in the header to slurm
NB_CPU=8

# Path to R script that sums across multiple realSFS chunks
rscript=/proj/snic2020-2-19/private/herring/users/khrystyna/Arctic_charr_ref_fSalAlp1/Nucleotide_diversity_NSITES/Rscripts/sum_sites_sfs.r

###############################################################
# Step 2: Loop over each BAM list
###############################################################
POP_2=$(cat $BAM_LIST/list_34chr.myv_tin | sed -n ${SLURM_ARRAY_TASK_ID}p)
POP=$(ls $BAM_LIST/$POP_2)
OUTPUT=$(basename "$POP" .txt)


    # Count number of individuals
    N_IND=$(wc -l < "$POP")

    # Define minimum proportion of individuals to keep per site
    if [ "$N_IND" -lt 12 ]; then
        PERCENT_IND=0.9
    else
        PERCENT_IND=0.5
    fi

    # Calculate minimum number of individuals to retain per site
    MIN_IND_FLOAT=$(echo "$N_IND * $PERCENT_IND" | bc -l)
    MIN_IND=${MIN_IND_FLOAT%.*}

    # Calculate maximum total depth threshold
    MAX_DEPTH=$(echo "$N_IND * $MAX_DEPTH_FACTOR" | bc -l)

    echo "--------------------------------------------"
    echo "Processing: $OUTPUT"
    echo "Number of individuals: $N_IND"
    echo "PERCENT_IND: $PERCENT_IND -> MIN_IND: $MIN_IND"
    echo "MAX_DEPTH: $MAX_DEPTH"
    echo "--------------------------------------------"

    ###############################################################
    # Step 3: Calculate genotype likelihoods and saf index
    ###############################################################

#Step 2: Finding a 'global estimate' of the SFS per each lake
angsd -P 20 -underFlowProtect 1 \
    -dosaf 1 -GL 2 -doMajorMinor 1 -doCounts 1 \
    -anc $REFGENOME -fai $REF_INDEXED \
    -rf $CHUNK_DIR/$CHUNK_NAMES_target \
    -remove_bads 1 -minMapQ 30 -minQ 20 -minInd $MIN_IND -setMaxDepth $MAX_DEPTH -setMinDepthInd $MIN_DEPTH -uniqueOnly 1 \
    -bam $POP \
    -out $OUT_DIR/${OUTPUT}_${CHUNK_NAMES_target_name}_pctind"$PERCENT_IND"_maxdepth"$MAX_DEPTH"



    ###############################################################
    # Step 4: Estimate real SFS from saf index
    ###############################################################

    echo "Estimating SFS for $OUTPUT..."
    realSFS $OUT_DIR/${OUTPUT}_${CHUNK_NAMES_target_name}_pctind"$PERCENT_IND"_maxdepth"$MAX_DEPTH".saf.idx -P $NB_CPU -nSites $NSITES  -maxIter 50 > $OUT_DIR/${OUTPUT}_${CHUNK_NAMES_target_name}_pctind"$PERCENT_IND"_maxdepth"$MAX_DEPTH"."$NSITES"


    ###############################################################
    # Step 5: Sum multiple SFS chunks using provided R script
    ###############################################################
file=$OUT_DIR/${OUTPUT}_${CHUNK_NAMES_target_name}_pctind"$PERCENT_IND"_maxdepth"$MAX_DEPTH"."$NSITES"
	
    Rscript $rscript "$file"


    ###############################################################
    # Step 6: Estimate thetas using the computed SFS
    ###############################################################

    echo "Estimating theta statistics for $OUTPUT..."
    angsd -P $NB_CPU -dosaf 1 -doThetas 1 -GL 2 -doMajorMinor 1 -underFlowProtect 1 \
    -anc $REFGENOME -fai $REF_INDEXED -remove_bads 1 -minMapQ 30 -minQ 20 -minInd $MIN_IND -setMinDepthInd $MIN_DEPTH \
    -rf $CHUNK_DIR/$CHUNK_NAMES_target \
	-bam $POP \
	-pest $OUT_DIR/${OUTPUT}_${CHUNK_NAMES_target_name}_pctind"$PERCENT_IND"_maxdepth"$MAX_DEPTH"."$NSITES".dsfs \
	-out $OUT_DIR/${OUTPUT}_${CHUNK_NAMES_target_name}_pctind"$PERCENT_IND"_maxdepth"$MAX_DEPTH"."$NSITES"
    

    ###############################################################
    # Step 7: Calculate per-scaffold and sliding window theta stats
    ###############################################################
    echo "Calculating theta summary statistics for $OUTPUT..."
    thetaStat do_stat $OUT_DIR/${OUTPUT}_${CHUNK_NAMES_target_name}_pctind"$PERCENT_IND"_maxdepth"$MAX_DEPTH"."$NSITES".thetas.idx
	
    echo "Performing sliding window theta scan..."
	thetaStat do_stat $OUT_DIR/${OUTPUT}_${CHUNK_NAMES_target_name}_pctind"$PERCENT_IND"_maxdepth"$MAX_DEPTH"."$NSITES".thetas.idx -win 20000 -step 10000 \
	-outnames $OUT_DIR/${OUTPUT}_${CHUNK_NAMES_target_name}_pctind"$PERCENT_IND"_maxdepth"$MAX_DEPTH"."$NSITES".thetaswindow
```	
	
	
	
# Step 2: Nucleotide diversity summary stats
```{r }
#Libs
library(tidyverse)
library(patchwork)
library(zoo)
library(data.table)

dir='~/Desktop/Comp_UU/REF_SalAlp_UK/Nucleotide_diversity/Myv_Thin_34chr'

#Load data 
files <- list.files(dir)

desired <- grep('.500000.thetaswindow.pestPG$',files, value = T)

#Colnames
col_names <- c('Chr',	'WinCenter',	'tW',	'tP',	'tF',	'tH',	'tL',	'Tajima', 'fuf', 'fud',	'fayh',	'zeng',	'nSites')


# Read the desired files
thetas_list <- list()

for(file in desired) {
  file_content <- read.table(file.path(dir, file), header = F)[,-1]
  colnames(file_content) <- col_names
  # Process the file content as needed
  thetas_list[[file]] <- file_content
}


#Modify names of the data list 
names(thetas_list) <- gsub("*.\\.list_34_pctind0.*_maxdepth.*500000\\.thetaswindow\\.pestPG$", '', names(thetas_list))


# Estimate Pi and Wattersonâ€™s theta  -------------------------------------------
l.theta <- list()
l.watt <- list()
l.sum <- list()
l.dfs <- list()

for (pop in seq_along(thetas_list)){
  df.theta <- thetas_list[[pop]]
  pop.name <- names(thetas_list[pop])
  l.theta[[pop.name]] <- mean((as.numeric(df.theta$tP) / as.numeric(df.theta$nSites)), na.rm = T)
  l.watt[[pop.name]] <- mean((as.numeric(df.theta$tW) /as.numeric(df.theta$nSites)), na.rm = T)
  l.sum[[pop.name]] <- sum(as.numeric(df.theta$nSites), na.rm = T)
  df.theta$pop <- pop.name
  l.dfs[[pop.name]] <- df.theta
}

length(l.theta) #as the number of populations
length(l.watt)  #as the number of populations

df.x2 <- data.frame(pop = names(l.theta),
                    pairwise.nuc = round(unlist(l.theta), 4), 
                    wattersons.theta = round(unlist(l.watt), 4),
                    number.sites = round(unlist(l.sum), 4))

mean.theta.nsites <- df.x2
df.theta.nsites <- bind_rows(l.dfs)
names(df.theta.nsites) <- c("Chr", "WinCenter","tW", "tP","tF","tH","tL","Tajima","fuf","fud","fayh","zeng","nSites", "pop")


# cleaner summary ---------------------------------------------------------
#always filter windows > 10 sites
df.theta.nsites$nSites <- as.numeric(df.theta.nsites$nSites)
df.theta.nsites$tP <- as.numeric(df.theta.nsites$tP)
df.theta.nsites$tW <- as.numeric(df.theta.nsites$tW)

# Plot nucleotide diversity-----------------------------------------------------
df.theta.nsites_withPi <- df.theta.nsites
df.theta.nsites_withPi$Pi <-round(df.theta.nsites_withPi$tP/df.theta.nsites_withPi$nSites, 4)
df.theta.nsites_withPi <- df.theta.nsites_withPi[!is.na(df.theta.nsites_withPi$Pi),]


#Use this data 
df.theta.nsites$WinCenter <- as.numeric(df.theta.nsites$WinCenter)
df.theta.nsites$WinCenter_less <- df.theta.nsites$WinCenter/1000000
df.theta.nsites$tP_per_site <- df.theta.nsites$tP/df.theta.nsites$nSites

#Clean data for the nSites , we are interested in nSites > 10
df.theta.nsites_clean <- df.theta.nsites[ df.theta.nsites$nSites > 10, ]

#Select for shared region only
df.theta.nsites_clean_shared <- df.theta.nsites_clean[df.theta.nsites_clean$WinCenter_less > 18.36 & df.theta.nsites_clean$WinCenter_less < 18.45 ,]

#Add summary statistics
library(dplyr)

summary_shar <- 
  df.theta.nsites_clean_shared %>%
  group_by(pop) %>%
  summarise(mean_pi = round(mean(tP_per_site),4),
            sd_pi = round(sd(tP_per_site),4))

print(summary_shar)
```

