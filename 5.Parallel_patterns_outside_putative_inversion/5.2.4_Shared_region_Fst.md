
# Step1: Compute Site Frequency Spectrum (SFS) and FST with ANGSD 
```{r}
#!/bin/bash -l

#SBATCH -A uppmax2025-2-114
#SBATCH -p core -n 20
#SBATCH -t 02-00:00:00
#SBATCH -J Fst_34chr
#SBATCH -e Fst_34chr_%A_%a.err
#SBATCH -o Fst_34chr_%A_%a.out
#SBATCH --mail-type=all
#SBATCH --mail-user=khrystyna.kurta@slu.se


module load bioinfo-tools
module load ANGSD/0.933
module load R_packages/4.3.1

PERCENT_IND=0.5  # Use 90% if <12 individuals
MIN_DEPTH=1
MAX_DEPTH_FACTOR=3
NB_CPU=20

# Paths
rscript=/proj/snic2020-2-19/private/herring/users/khrystyna/Arctic_charr_ref_fSalAlp1/FST_MAF0.05_FOLDED/Rscripts/01_sum_sites_2dsfs.r
CHUNK_DIR=/proj/snic2020-2-19/private/herring/users/khrystyna/Arctic_charr_ref_fSalAlp1/CHUNK_LIST
BASEDIR=/proj/snic2020-2-19/private/herring/users/khrystyna/Arctic_charr_ref_fSalAlp1/Run_1_2
CHUNK_NAMES_target=34.txt
CHUNK_NAMES_target_name=${CHUNK_NAMES_target/.txt/}
POP_LIST=/proj/snic2020-2-19/private/herring/users/khrystyna/Arctic_charr_ref_fSalAlp1/BAM_LISTS/SHARED_list/Shared_34chr/separate_morph_homo
BAM_LIST=/proj/snic2020-2-19/private/herring/users/khrystyna/Arctic_charr_ref_fSalAlp1/BAM_LISTS/SHARED_list/Shared_34chr
SITES=/proj/snic2020-2-19/private/herring/users/khrystyna/Arctic_charr_ref_fSalAlp1/SITES/sites_sorted.txt
OUT_DIR=/proj/snic2020-2-19/private/herring/users/khrystyna/Arctic_charr_ref_fSalAlp1/FST_chr34
NSITES=500000

REFDIR=/proj/snic2020-2-19/private/arctic_charr/assemblies/fSalAlp1.1/Data_Package_fSalAlp1_assembly_20231024
REFGENOME=$REFDIR/fSalAlp1.1.hap1.cur.20231016.fasta
REF_INDEXED=$REFDIR/fSalAlp1.1.hap1.cur.20231016.fasta.fai

cd $BASEDIR

########################################
# Step 1: Calculate SAF Files for Each Population
########################################

for POP in $(ls $BAM_LIST/Myv_homo*list $BAM_LIST/Thin_homo*list $BAM_LIST/Myv_Thin_homo*.list); do
    OUTPUT=$(basename $POP)
    N_IND=$(cat $POP | wc -l)

    if [ "$N_IND" -lt 12 ]; then
      PERCENT_IND=0.9
    else
      PERCENT_IND=0.5
    fi

    MIN_IND_FLOAT=$(echo "($N_IND * $PERCENT_IND)" | bc -l)
    MIN_IND=${MIN_IND_FLOAT%.*}

    echo "Working on pop $POP, $N_IND individuals"
    echo "Using PERCENT_IND=$PERCENT_IND, which corresponds to at least $MIN_IND individuals with data per site"

    angsd -P $NB_CPU \
        -doSaf 1 -GL 2 -doMajorMinor 5 \
        -anc $REFGENOME -fai $REF_INDEXED \
        -minMapQ 30 -minQ 20 -remove_bads 1 \
        -setMinDepthInd $MIN_DEPTH -minInd $MIN_IND \
        -rf $CHUNK_DIR/$CHUNK_NAMES_target \
        -bam $POP -out $OUT_DIR/${OUTPUT}_${CHUNK_NAMES_target_name}
done

########################################
# Step 2: Compute 2D SFS and FST for All Pairwise Contrasts
########################################

cd $OUT_DIR
populations=($(cat $POP_LIST))

for ((i = 0; i < ${#populations[@]} - 1; i++)); do
  for ((j = i + 1; j < ${#populations[@]}; j++)); do
      pop_1_name="${populations[$i]}"
      pop_2_name="${populations[$j]}"

      echo "Comparing pairs: $pop_1_name and $pop_2_name at $CHUNK_NAMES_target_name"

      realSFS "${pop_1_name}_${CHUNK_NAMES_target_name}.saf.idx" \
              "${pop_2_name}_${CHUNK_NAMES_target_name}.saf.idx" \
              -P $NB_CPU -maxIter 30 -nSites $NSITES > \
              "${pop_1_name}.${pop_2_name}.${CHUNK_NAMES_target_name}.$NSITES"

      file="${pop_1_name}.${pop_2_name}.${CHUNK_NAMES_target_name}.$NSITES"
      Rscript $rscript "$file"

      realSFS fst index "${pop_1_name}_${CHUNK_NAMES_target_name}.saf.idx" \
                        "${pop_2_name}_${CHUNK_NAMES_target_name}.saf.idx" \
                        -sfs ${pop_1_name}.${pop_2_name}.${CHUNK_NAMES_target_name}.${NSITES}.2dsfs \
                        -P $NB_CPU \
                        -fstout "${pop_1_name}.${pop_2_name}.${CHUNK_NAMES_target_name}.$NSITES"

      realSFS fst print "${pop_1_name}.${pop_2_name}.${CHUNK_NAMES_target_name}.$NSITES".fst.idx \
                        -P $NB_CPU > \
                        "${pop_1_name}.${pop_2_name}.${CHUNK_NAMES_target_name}.$NSITES".bypos.sfs

      realSFS fst stats "${pop_1_name}.${pop_2_name}.${CHUNK_NAMES_target_name}.$NSITES".fst.idx \
                        -P $NB_CPU > \
                        "${pop_1_name}.${pop_2_name}.${CHUNK_NAMES_target_name}.$NSITES".fst

      realSFS fst stats2 "${pop_1_name}.${pop_2_name}.${CHUNK_NAMES_target_name}.$NSITES".fst.idx \
                         -win 20000 -step 10000 -P $NB_CPU > \
                         "${pop_1_name}.${pop_2_name}.${CHUNK_NAMES_target_name}.${NSITES}_20kb_10kbstep.slidingwindow"

      realSFS fst stats2 "${pop_1_name}.${pop_2_name}.${CHUNK_NAMES_target_name}.$NSITES".fst.idx \
                         -win 25000 -step 5000 -P $NB_CPU > \
                         "${pop_1_name}.${pop_2_name}.${CHUNK_NAMES_target_name}.${NSITES}_25kb_5kbstep.slidingwindow"
  done
done
```



## R script used to estimate 2sfs
```{r}
01_sum_sites_2dsfs.r
# code R to take the sfs made on a subsample of sites in a usable format for subsequent analyses
argv <- commandArgs(T)
file<-argv[1]


sfs<-read.table (paste0(file))

sfs.sum<-colSums(sfs)
write.table(rbind(sfs.sum),  quote=F, col.names=F, row.names=F,paste0(file, ".2dsfs"))
```



# Step 2: Calcultae Fst statistics
```{r}
#Libs
library(tidyverse)
library(patchwork)
library(zoo)
library(data.table)
library(dplyr)
library(scales)
library(ggplot2)
options(scipen = 999)

#Directory
dir <- '~/Desktop/Comp_UU/REF_SalAlp_UK/FST/Myv_Thin_chr34'

#Load data 
files <- list.files(dir)

# Filter files that start with numbers from 1 to 13 and end with ""
desired_files <- grep("_20kb_10kbstep.slidingwindow$", files, value = TRUE)
col_names <- c('chr', 'midPos','Nsites', 'Fst')


# Read the desired files
fst_list <- list()

for(file in desired_files) {
  # Try to read the file content, handle any errors gracefully
  file_content <- try(read.table(file.path(dir, file), header = TRUE, sep = "\t", stringsAsFactors = FALSE), silent = TRUE)
  
  # Check if the read operation resulted in an error or if the file is empty
  if (inherits(file_content, "try-error") || nrow(file_content) == 0) {
    next  # Skip to the next file in the loop
  }
  
  #file_content <- read.table(file.path(dir, file), header = F)[,-1]
  colnames(file_content) <- col_names
  
  #Names of pops
  pops <- gsub("maf_|\\.list.maf_|\\.list.|[1-9]|\\_20kb_10kbstep.slidingwindow", "", file)
  pops <- gsub("0", "", pops)
  
  file_content$Populations <- pops

  # Process the file content as needed
  fst_list[[file]] <- file_content
}

#Combine and filter for > 10 sites
fst_combined <- do.call(rbind, fst_list)
fst_combined_filter <- fst_combined[fst_combined$Nsites >= 10,]

#Select only shared region 
x1 = 18360000
x2 = 18450000

fst_combined_filter_shared <- fst_combined_filter[fst_combined_filter$midPos >=x1 & fst_combined_filter$midPos <=x2, ]

#Check if all chromosomes are present
fst_stats <- fst_combined_filter_shared %>%
                group_by(Populations) %>%
                summarise(n_chr = n_distinct(chr),
                          n_sites = sum(Nsites),
                          mean_Fst = round(mean(Fst),2 ),
                          sd_fst = sd(Fst)
)

View(fst_stats)
```



